# Redis场景

## Redis集群

### Redis cluster

- 自动将数据进行分片，每个master上放一部分数据。
- 提供内置的高可用支持，部分master不可用时，还是可以继续工作的。

再Redis cluster架构下，每个Redis要放开两个端口号，一个是6379，另一个是加1万的端口号，比如16379。16379用来进行节点间通信，也就是cluster bus通信，用来进行故障检测、配置更新、故障转移授权。cluster bus使用gossip协议，进行节点间高效的数据交换，占用更少的网络带宽和处理时间。

### 节点间内部通信机制

#### 基本通信原理

集群元数据的维护有两种方式：集中式、Gossip协议。

集中式是将集群元数据（节点信息、故障等等）集中存储在某个节点上。集中式元数据集中存储的一个典型代表，就是大数据领域的storm。是分布式的大数据实时计算引擎，是集中式的元数据存储的结构，底层基于zookeeper（分布式协调的中间件）对所有元数据进行存储维护。

Goosip协议是所有节点都持有一份元数据，不同的节点如果出现了元数据的变更，就不断将元数据发送给其他节点，让其他节点进行元数据的变更。

#### 集中式

- 优点：元数据的读取和更新，时效性非常好，一旦元数据出现了变更，就立即更新到集中式的存储中，其他节点读取的时候就可以感知到。
- 缺点：所有的元数据的更新压力全部集中在一个地方，可能会导致元数据的存储有压力。

#### Gossip协议

- 优点：元数据更新比较分散，不是集中在一个地方，更新请求会陆陆续续打到所有节点上去更新，降低了压力。
- 缺点：元数据的更新有延时，可能导致集群中的一些操作会有些滞后。

### 分布式寻址算法

- hash算法（大量缓存重建）
- 一致性hash算法（自动缓存迁移） + 虚拟节点（自动负载均衡）
- Redis cluster的hash slot算法

#### hash算法

请求过来一个key，首先计算hash值，然后对节点数取模。根据取模后的结果打到不同的master节点上。一旦某个master节点宕机，所有请求过来都会基于最新的剩余master节点数去取模，尝试获取数据。这会导致大部分的请求过来，无法拿到有效的缓存，导致大量请求涌入数据库。

#### 一致性hash算法

一致性hash算法将整个hash值空间组织称一个虚拟圆环，整个空间按顺时针方向组织，将各个master节点（使用服务器的ip或主机名）进行hash。就能确定每个节点在哈希环上的位置。来了一个key，首先计算hash值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，遇到的第一个master节点就是key所在位置。

在一致性哈希算法中，如果一个节点挂了，受影响的数据仅仅是此节点到环空间前一个节点（沿着逆时针方法行走遇到的第一个节点）之间的数据，其他数据不受影响。增加一个节点也同理。

一致性哈希算法在节点太少时，容易因为节点分布不均匀而造成缓存热点的问题。为了解决这种热点问题，一致性hash算法引入了虚拟节点机制，即对每个节点计算多个hash，每个计算位置都放置一个虚拟节点。实现了数据的均匀分布，负载均衡。

#### Redis cluster的hash slot算法

Redis cluster有固定的16384个hash slot，对每个key计算CRC16值，然后对16384取模，可以获取key对应的hash slot。每个master会持有部分slot，比如有3个master，那么每个master持有5000多个hash slot。hash slot让node的增加和移除很简单，增加一个master就将其他master的hash slot移动部分过去，减少一个master就将它的hash slot移动到其他master上去。移动hash slot的成本非常低，客户端的api，可以对指定的数据，让他们走同一个hash slot，通过hash tag来实现。

### Redis高可用与主备切换原理

#### 判断节点宕机

如果一个节点认为另一个节点宕机，那么就是pfail，主观宕机。如果多个节点都认为另外一个节点宕机了，那么就是fail，客观宕机。跟哨兵原理及户一样，sdown，odown。

在cluster-node-timeout内，某个节点一直没有返回pong，那么就被认为pfail。如果一个节点认为某个节点pfail了，那么会在gossip ping消息中，ping给其他节点，如果超半数的节点都认为pfail了，那么就会变成fail。

#### 从节点过滤

对宕机的master node，从其所有的slave node中，选择一个切换成master node。检查每个slave node与master node断开连接的时间，如果超过了cluster-node-timeout * cluster-slave-validity-factor，那么就没资格切换成master。

#### 从节点选举

每个从节点都根据自己对master复制数据的offset，设置一个选举时间，offset越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举。

所有的master node开始slave选举投票，给要进行选举的slave进行投票，如果大部分master node （N/2+1）都投票给某个从节点，那么选举通过，那个从节点切换成master。

#### 与哨兵比较

整个流程跟哨兵相比，非常类似。

## Redis一致性

### Cache Aside Pattern

最经典的缓存+数据库读写的模式，就是Cache Aside Pattern。

- 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据放入缓存，同时返回响应。
- 更新的时候，先更新数据库，然后再删除缓存。

#### 为什么是删除缓存而不是更新缓存

在复杂缓存场景中，缓存不单单是数据库直接取出来的值，可能是经过计算后的值。

更新缓存的代价有时候很高，对于比较复杂的缓存计算的场景，如果频繁修改一个缓存涉及的多个表，缓存也频繁更新。但是这个缓存是否会被频繁访问到。

删除缓存而不是更新缓存，是一个lazy计算的思想，不需要每次都重新做复杂的计算，不管会不会用到，而是让它到需要被使用的时候再重新计算。

### 初级缓存不一致问题及解决方案

问题：先更新数据库，再删除缓存。如果删除缓存失败了，会导致数据库中是最新数据，缓存中是旧数据，数据就出现了不一致。

解决思路1：先删除缓存，再更新数据库。如果数据库更新失败，那么数据中是旧数据，缓存中是空的，那么数据不会不一致。读的时候缓存没有，去读了数据库中的旧数据，然后再更新到缓存中。

解决思路2：延时双删。先更新数据库，再删除缓存。不同的是，把删除的动作，在不久后执行一次，比如5s后。

### 比较复杂的数据不一致问题

问题：数据发生变更，先删除了缓存，然后去修改数据库，此时数据还没修改。另一个请求过来，查询缓存没有，去查询数据库，查到了修改前的旧数据，放到了缓存中。随后数据变更的程序完成了数据库的修改。数据库和缓存中的数据不一致了。

#### 方案一：队列串行化

适用场景：金融交易、库存扣减等强一致性场景。

核心：将对同一数据的读写操作串行化。

方案流程：

1. 写线程删除缓存
2. 写线程发送更新DB任务
3. 读线程读缓存（未命中）
4. 读线程发送读DB+写缓存任务
5. 队列执行写线程的DB更新
6. 队列执行读线程的DB查询
7. 队列写入新数据到缓存
8. 读线程从缓存获取新数据

关键实现：

- 使用分布式队列
- 按数据ID做路由分区（相同ID发到同一分区）
- 消费者单线程处理分区消息（保证顺序性）
- 读请求去重：分区内合并相同ID的读请求
- 超时熔断：等待超时后直连DB（不写缓存）

优势：彻底解决并发冲突

代价：增加延迟，架构复杂度高

#### 方案二：订阅数据库binlog（最终一致性）

适用场景：电商、社交等可接受秒级延迟的场景

核心：通过数据库日志异步更新缓存

方案流程：

1. 写服务：删除缓存 -> 更新DB
2. 启动binlog订阅服务（Canal）
3. 解析UPDATE事件，发送到消息队列
4. 消费者获取新数据，执行redis.set(key, new_value)，或使用lua脚本保证原子性

优势：

- 读写服务完全解耦
- 缓存更新失败可重试
- 天然支持批量更新

#### 方案三：延时双删 + 重试（低成本）

适用场景：中小流量系统，无中间件资源

方案流程：

1. 写服务第一次删除缓存
2. 写服务更新数据库
3. 写服务第二次延时删除缓存

增强措施：

1. 异步重删失败时，写入重试队列
2. 读请求发现缓存空时，短暂互斥锁重建缓存
3. 缓存值带版本号，写时校验版本

#### 方案四：lua脚本 + 最终一致性

核心思路：

1. 版本号机制：所有数据增加版本号（或时间戳）
2. 原子操作：使用Lua脚本保证缓存操作的原子性
3. 异步校验：通过后台任务或消息队列修正不一致
4. 最终一致：接受毫米级短暂不一致，确保最终一致

异步校验流程：

1. 消息队列发送版本变更事件
2. 校验Worker从Redis中获取缓存和版本
3. 校验Worker查询当前数据库版本
4. 校验Worker对比缓存和数据库版本，执行Lua更新脚本
5. 校验Worker向消息队列确认消息

优势：

1. 高性能：读操作只增加1次版本比较
2. 低延迟：写操作无需等待校验完成
3. 安全可靠：Lua保证原子操作，消息队列保证可靠性
4. 自修复：异步校验自动修正不一致
5. 资源可控：校验频率可动态调整
