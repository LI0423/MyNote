# 扩展卡尔曼滤波（Extended Kalman Filter，EKF）

当动力学或观测模型中含非线性时，不能直接用线性卡尔曼滤波器。通过在当前估计点对非线性函数做一阶泰勒线性化（求雅可比矩阵），把问题在每步近似成线性卡尔曼滤波来处理。适用于：稍微非线性、噪声近似高斯、需要在线计算的场景（如机器人定位、航迹跟踪等）。

## 算法步骤

### 预测（predict）

1. 状态预测
    $$\hat{X}_{k|k-1} = f(\hat{X}_{k-1|k-1}, u_k)$$
2. 协方差预测
    $$P_{k|k-1} = F_j P_{k-1|k-1} F_j^T + Q_k$$

### 更新（Update）

1. 计算创新（残差）
    $$y_k = z_k - h(\hat{X}_{k|k-1})$$
2. 创新协方差
    $$S_k = H_j P_{k|k-1} H_j^T + R_k$$
3. 卡尔曼增益
    $$K_k = P_{k|k-1} H_k^T S_k^{-1}$$
4. 状态修正
    $$\hat{X}_{k|k-1} = \hat{X}_{k|k-1} + K_k y_k$$
5. 协方差修正
    $$P_{k|k} = (I - K_k H_k) P_{k|k-1}$$
    更数值稳定的Joseph表达式：
    $$P_{k|k} = (I - K_k H_k)P_{k|k-1}(I - K_k H_k)^T + K_k R_k K_k^T$$

## 雅可比矩阵

雅可比矩阵把非线性系统在当前估计点做一次一阶线性化，用于把状态误差与观测/下一步状态的线性关系联结起来，从而让我们能用线性卡尔曼滤波的协方差传播与卡尔曼增益公式去估计误差协方差并计算更新。

### 问题

- 在标准KF中，状态转移和观测都是线性的（由矩阵F和H表示）。线性变换有一个完美的性质：一个高斯分布经过线性变换后，仍然是高斯分布。因此可以直接用矩阵$F$和$H$来变换协方差矩阵：
  - $P_k^- = F * P_{k-1} * F^T$（状态不确定性传播）
  - $S = H * P_k^- * H^T$（将状态不确定性映射到观测空间）
- 在EKF中，系统是非线性的（由函数$f$和$h$表示）。一个高斯分布经过非线性变换后，不再保持高斯分布，其形状会变得扭曲和不规则。无法直接将非线性函数$f$或$h$作用于协方差矩阵$P$上。

### 传播状态不确定性（预测步骤）

- 状态预测：直接使用非线性函数$f$。$\hat{X}_K^- = f(\hat{X}_{k-1}, u_k)$
- 协方差预测：不能使用$f$。使用$f$的雅可比矩阵$F_j$，在当前估计点$\hat{x}_{k-1}$处计算。$P_k^- = F_j * P_{k-1} * F_j^T + Q$
  - 为什么？
  - 雅可比矩阵$F_j$描述了非线性状态函数$f$在$\hat{X}_{k-1}$这个点附近的最佳线性逼近。捕捉了系统模型在局部区域的敏感度和变化趋势。实际上就是在$\hat{X}_{k-1}$这个点附近，暂时假装系统是线性的，其状态转移矩阵就是这个$F_j$。这样就可以利用标准KF的线性协方差更新公式，近似地、局部地传播不确定性。

### 将状态不确定性映射到观测空间（更新步骤）

- 观测预测：直接使用非线性函数$h$。$\hat{z}_k = h(\hat{x}_k^-)$。
- 观测不确定性计算：不能使用$h$。使用$h$的雅可比矩阵$H_j$，在预测状态点$\hat{x}_k^-$处计算。$$S_k = H_j P_{k|k-1} H_j^T + R_k$$
  - 为什么？
  - 雅可比矩阵$H_j$描述了非线性观测函数$h$在$\hat{x}_k^-$这个点附近的最佳线性逼近。状态空间的微小变化会如何引起观测空间的微小变化。$H_j * P_k^- * H_j^T$正是利用局部线性模型，将状态的不确定性$P_k^-$“翻译”成了观测的不确定性。

### 计算最优卡尔曼增益（更新步骤）

卡尔曼增益决定了应该相信预测模型多一点，还是相信观测数据多一点。计算也依赖于线性化。$K_k = P_k^- * H_j^T * S^{-1}$。正是通过雅可比矩阵$H_j$，才能建立起状态空间和观测空间之间的关联，从而计算出这个最优的权重。

### 直观类比：用切线近似曲线

想象你在一片崎岖的非线性山地（复杂系统）中蒙眼行走，你的目标是找到最低点（最优状态）。

- 你（EKF）： 每走一步就停下来，用脚感受一下周围的地面（计算当前点的雅可比矩阵，即坡度）。
- 雅可比矩阵： 就是你感受到的“当前所在地的坡度信息”。它告诉你，在你站立的这个极小区域内，地面可以近似看作一个斜面（线性化）。
- 你的决策： 你根据这个“局部斜面”的信息（F_j, H_j），决定下一步往哪个方向走、走多远（计算 P_k⁻ 和 K_k，更新状态 x̂_k）。

你不会试图去感知整个复杂的地形（全局非线性模型），而是每一步都重新感知局部地形，并基于这个局部信息做出最优决策。这就是EKF和雅可比矩阵工作的精髓——不断的局部线性化。

## 代码示例

```python
# Python code to implement and demonstrate an Extended Kalman Filter (EKF).
# The code simulates a simple differential-drive robot in 2D:
#   state: [x, y, theta]
#   control: [v, omega] (linear and angular velocities)
#   motion model: discrete-time integration
#   observation: range and bearing to a known landmark
#
# It runs a simulation, applies EKF, and plots:
#  - True vs estimated trajectory
#  - Position error magnitude over time
#
# Note: matplotlib is used for plotting (no seaborn).

import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)


def wrap_angle(a):
    """Wrap angle to [-pi, pi)."""
    return (a + np.pi) % (2 * np.pi) - np.pi


class EKF:
    def __init__(self, f, F_jac, h, H_jac, Q, R, x0, P0):
        """
        f: state transition function f(x, u)
        F_jac: function to compute Jacobian of f with respect to x: F = F_jac(x, u)
        h: observation function h(x)
        H_jac: function to compute Jacobian of h wrt x: H = H_jac(x)
        Q: process noise covariance (n x n)
        R: observation noise covariance (m x m)
        x0: initial state (n,)
        P0: initial covariance (n x n)
        """
        self.f = f
        self.F_jac = F_jac
        self.h = h
        self.H_jac = H_jac
        self.Q = Q
        self.R = R
        self.x = x0.copy()
        self.P = P0.copy()

    def predict(self, u):
        # Predict state and covariance
        self.x = self.f(self.x, u)
        F = self.F_jac(self.x, u)
        self.P = F @ self.P @ F.T + self.Q

    def update(self, z):
        # Standard EKF update
        H = self.H_jac(self.x)
        z_pred = self.h(self.x)
        y = z - z_pred  # innovation
        # If observation has angles, wrap them. Here z[1] is bearing.
        if y.shape[0] >= 2:
            y[1] = wrap_angle(y[1])

        S = H @ self.P @ H.T + self.R
        K = self.P @ H.T @ np.linalg.inv(S)
        self.x = self.x + K @ y
        # Wrap angle in state
        self.x[2] = wrap_angle(self.x[2])
        I = np.eye(self.P.shape[0])
        # Joseph form for numerical stability
        self.P = (I - K @ H) @ self.P @ (I - K @ H).T + K @ self.R @ K.T

        # return the NIS (normalized innovation squared) for diagnostics
        nis = float(y.T @ np.linalg.inv(S) @ y)
        return nis


# ---------- Model definitions for the simple robot ----------

def motion_model(x, u, dt=0.1):
    """Discrete motion model (unicycle-like)"""
    px, py, th = x
    v, omega = u
    if abs(omega) < 1e-6:
        # approximate straight-line integration
        px_new = px + v * dt * np.cos(th)
        py_new = py + v * dt * np.sin(th)
        th_new = th
    else:
        # exact integration for constant velocities over dt (useful if omega != 0)
        px_new = px + (v / omega) * (np.sin(th + omega * dt) - np.sin(th))
        py_new = py - (v / omega) * (np.cos(th + omega * dt) - np.cos(th))
        th_new = th + omega * dt
    return np.array([px_new, py_new, wrap_angle(th_new)])


def jacobian_F(x, u, dt=0.1):
    """Jacobian of motion model wrt state x evaluated at (x,u)"""
    _, _, th = x
    v, omega = u
    if abs(omega) < 1e-6:
        # Straight-line approximation derivatives
        F = np.array([
            [1, 0, -v * dt * np.sin(th)],
            [0, 1,  v * dt * np.cos(th)],
            [0, 0, 1]
        ])
    else:
        # Derivatives for exact integration
        th2 = th + omega * dt
        F = np.array([
            [1, 0, (v / omega) * (np.cos(th2) - np.cos(th))],
            [0, 1, (v / omega) * (np.sin(th2) - np.sin(th))],
            [0, 0, 1]
        ])
    return F


# Observation: range and bearing to a known landmark l = (lx, ly)
def observe(x, landmark):
    px, py, th = x
    lx, ly = landmark
    dx = lx - px
    dy = ly - py
    rng = np.sqrt(dx * dx + dy * dy)
    bear = wrap_angle(np.arctan2(dy, dx) - th)
    return np.array([rng, bear])


def jacobian_H(x, landmark):
    px, py, th = x
    lx, ly = landmark
    dx = lx - px
    dy = ly - py
    q = dx * dx + dy * dy
    sqrtq = np.sqrt(q)
    # H is 2x3: partial derivatives of [range, bearing] wrt [px,py,th]
    H = np.zeros((2, 3))
    H[0, 0] = -dx / sqrtq
    H[0, 1] = -dy / sqrtq
    H[0, 2] = 0.0
    H[1, 0] = dy / q
    H[1, 1] = -dx / q
    H[1, 2] = -1.0
    return H


# ---------- Simulation setup ----------
dt = 0.1
T = 200  # number of steps
landmark = np.array([5.0, 5.0])  # a single known landmark

# True initial state
x_true = np.array([0.0, 0.0, 0.0])

# Initial belief
x0 = np.array([0.1, -0.1, 0.05])  # slightly offset initial guess
P0 = np.diag([0.5, 0.5, 0.1])

# Process and measurement noise covariances
Q = np.diag([0.01, 0.01, 0.001])   # process noise covariance
R = np.diag([0.1**2, (np.deg2rad(2))**2])  # measurement noise: range (m) and bearing (rad)

ekf = EKF(
    f=lambda x, u: motion_model(x, u, dt=dt),
    F_jac=lambda x, u: jacobian_F(x, u, dt=dt),
    h=lambda x: observe(x, landmark),
    H_jac=lambda x: jacobian_H(x, landmark),
    Q=Q,
    R=R,
    x0=x0,
    P0=P0
)

# storage for plotting and diagnostics
xs_true = np.zeros((T, 3))
xs_est = np.zeros((T, 3))
Ps = np.zeros((T, 3, 3))
nises = np.zeros(T)

# simulate controls (circle-ish motion)
v_cmd = 0.5  # constant forward speed
omega_cmd = 0.15  # constant yaw rate

for k in range(T):
    # Simulate true motion with process noise
    u = np.array([v_cmd, omega_cmd])
    # add process noise to true motion (sampled in state space approximated by small gaussian on state)
    # We add noise to control to create variety, roughly representing model uncertainty
    u_noisy = u + np.random.multivariate_normal(mean=np.zeros(2), cov=np.diag([0.02, 0.005]))
    x_true = motion_model(x_true, u_noisy, dt=dt)
    xs_true[k] = x_true

    # Generate noisy observation
    z_true = observe(x_true, landmark)
    z = z_true + np.random.multivariate_normal(mean=np.zeros(2), cov=R)

    # EKF predict (using nominal control without noise)
    ekf.predict(u)
    # EKF update with measurement z
    nis = ekf.update(z)

    xs_est[k] = ekf.x
    Ps[k] = ekf.P
    nises[k] = nis

# ---------- Plots ----------
# 1) Trajectories
plt.figure(figsize=(7, 6))
plt.plot(xs_true[:, 0], xs_true[:, 1], label="True trajectory")
plt.plot(xs_est[:, 0], xs_est[:, 1], label="EKF estimate", linestyle="--")
plt.scatter([landmark[0]], [landmark[1]], marker="x", label="Landmark")
plt.xlabel("x (m)")
plt.ylabel("y (m)")
plt.legend()
plt.title("True vs EKF estimated trajectory")
plt.axis("equal")
plt.grid(True)
plt.show()

# 2) Position error magnitude over time
pos_err = np.linalg.norm(xs_true[:, :2] - xs_est[:, :2], axis=1)
plt.figure(figsize=(7, 4))
plt.plot(pos_err)
plt.xlabel("step")
plt.ylabel("position error (m)")
plt.title("Position error magnitude over time")
plt.grid(True)
plt.show()

# 3) Print some diagnostics about NIS
print("NIS mean:", np.mean(nises))
print("NIS std:", np.std(nises))
print("Final state estimate:", ekf.x)
print("Final covariance P:\n", ekf.P)
```
