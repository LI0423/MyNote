# 损失函数（Loss Function）

损失函数，也称为代价函数。是一个衡量模型预测值（Prediction）与真实值（True Value）之间差异程度的函数。为模型提供反馈信号，告诉模型现在差距有多大，从而指引模型应该向哪个方向、多大的力度去调整（通过优化算法）。

## 常见损失函数及应用场景

### 用于回归任务（预测连续值）

1. 均方误差（MSE）
   $$\frac{\sum_{i=1}^{N}(y_i - \hat{y}_i)^2}{N}$$
2. 平均绝对误差（MAE）
   $$\frac{\sum_{i=1}^{N}|y_i - \hat{y}_i|}{N}$$
3. Huber Loss

### 用于分类任务（预测离散类别）

1. 交叉熵损失（Cross-Entropy）
    $$-\frac{\sum_{i=1}^{N}y_i \log(\hat{y}_i)}{N}$$
2. 二元交叉熵（BCE）
    $$-\frac{\sum_{i=1}^{N} [y_i \log(\hat{y}_i) + (1- y_i)\log(1-\hat{y}_i)]}{N}$$
3. 合叶损失（Hinge Loss）
    $$\frac{\sum_{i=1}^{N}\max(0, 1 - y_i \cdot \hat{y}_i)}{N}， y_i\in{(-1, 1)}$$

## 多分类交叉熵损失函数（Categorial Cross-Entropy）

比较两个概率分布的差异。损失值越小，说明预测分布与真实分布越接近。

### 公式

$$L = -\sum_{i=1}^C y_i · \log(\hat{y_i})$$

- $C$：类别的总数。
- $y_i$：真实标签中第$i$个类别的值（要么是0，要么是1）。
- $\hat{y}_i$：模型预测出的第$i$个类别的概率（经过Softmax后）。
- $\log$：通常指自然对数（ln）。

### 示例

如果真实标签是：[1, 0, 0]代表“猫”，而预测概率为：[0.659, 0.242, 0.099]。
将真实分布和预测概率代入公式得到：
$L = -[1 · \log(0.659) + 0 · \log(0.242) + 0 · \log(0.009)] = - \log(0.659) \approx 0.417$

### 为何多分类交叉熵如此有效

假设有一个真实的类别 $k$（即$y_k = 1$），模型对它的预测概率是 $\hat{y}_k$。经过推导后，损失函数$L$关于第$j$个类别的logit（$z_j$，即Softmax的输入）的梯度为：
$\frac{\delta L}{\delta z_j} = \hat{y_j} - y_j$

这个公式意味着，梯度 = 预测值 - 真实值。指出了参数需要调整的方向和幅度。

- 对于真实类别$k$：梯度是$(\hat{y}_k - 1)$，这是一个负数，告诉网络需要增加这个类别的logit值。
- 对于其他所有错误类别$j$：梯度是 $(\hat{y}_j - 0) = \hat{y}_j$。这是一个正数，告诉网络需要减少这些类别的logit值。

### 优点

1. 梯度永不饱和：无论预测概率是0.99还是0.01，梯度$(\hat{y}_j - y_j)$都是一个合理值，不会变得极小。彻底解决Sigmoid等函数带来的梯度消失问题。
2. 收敛速度快：误差信号非常直接和巨大。模型如果犯了一个明显的错误（如$\hat{y}_{true} = 0.1$），会立刻收到一个很大的梯度$(-0.9)$，从而进行大幅度的参数更新来快速修正错误。
