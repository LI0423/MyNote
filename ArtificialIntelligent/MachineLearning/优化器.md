# 优化器

通过计算损失函数关于模型参数（权重和偏置）的梯度，来指导如何更新这些参数，从而最小化损失函数。

## 常见优化器

### 动量法

引入“动量”的概念，类似于物理学中的惯性。参数更新不仅考虑当前的梯度，还会累积之前的梯度方向。

- 工作机制：通过一个衰减系数$\gamma$（通常为0.9）来积累过去的梯度。如果连续几次梯度方向一致，动量会越来越大，加速在这个方向的移动；如果方向改变，动量会使其减弱，从而抑制震荡。
- 公式：
    $$v_t=\gamma v_{t-1} + \alpha · \nabla J(W,b)$$
    $$W = W - v_t$$
- 优点：
  - 加速在稳定方向的收敛。
  - 减少震荡，帮助闯过狭窄的山谷和局部最小值。

### AdaGrad

自适应学习率。为每个参数赋予不同的学习率。对于频繁更新的参数（对应特征的梯度大），降低其学习率；对于不频繁更新的参数（对应特征的梯度小），增加其学习率。适合处理稀疏数据。

- 工作机制：累积历史所有梯度的平方和。参数更新时，学习率会处以这个累积和的平方根。
- 公式：
    $$G_t = G_{t-1} + (\nabla J(W, b))^2$$
    $$W = W - \frac{\alpha}{\sqrt{G_t} + \epsilon} · \nabla J(W, b)$$ （$\epsilon$是一个极小值，防止除数为零）
- 缺点：由于$G_t$一直在累加，分母会越来越大，导致学习率过早且过量地减少，最终可能使模型在训练早期就停止学习。

### RMSProp

对AdaGrad的改进，解决其学习率急剧下降的问题。

- 工作机制：不像AdaGrad那样累积所有历史梯度平方，而是引入一个衰减系数$\beta$（如0.9），只关注最近一段时间的梯度平方，类似于移动平均。避免了分母无限增长。
- 公式：
    $$E[g^2]_t=\beta E[g^2]_{t-1} + (1-\beta)(\nabla J(W, b))^2$$
    $$W = W - \frac{\alpha}{\sqrt{E[g^2]_t} + \epsilon · \nabla J(W,b)}$$
- 优点：是AdaGrad的一个非常有效的升级，在实践中表现良好。

### Adam

自适应矩估计，结合了动量法和RMSProp的优点。即同时考虑了梯度的一阶矩（均值，提供动量）和二阶矩（未中心化的方差，自适应学习率）。

- 工作机制：
  1. 计算梯度的一阶矩（动量）的指数移动平均$m_t$。
  2. 计算梯度的二阶矩的指数移动平均$v_t$（类似于RMSProp）。
  3. 对$m_t$和$v_t$进行偏差校正，因为在训练初期，偏向于0。这一步非常重要。
  4. 使用校正后的值来更新参数。
- 公式：
    $$m_t=\beta_1 m_{t-1} + (1-\beta_1) \nabla J(W,b)$$
    $$v_t=\beta_2 v_{t-1} + (1-\beta_2)(\nabla J(W, b))^2$$
    $$\hat{m_t}=\frac{m_t}{1-\beta^t_1}$$
    $$\hat{v_t}=\frac{v_t}{1-\beta^t_2}$$
    $$W=W-\frac{\alpha}{\sqrt{\hat{v_t}}+\epsilon}\hat{m_t}$$
- 优点：
  - 通常收敛速度快。
  - 对超参数选择相对鲁棒（默认参数$\beta_1=0.9,\beta_2=0.999,\epsilon=10^{-8}$通常效果就很好）。
  - 适用于大多数非凸优化问题，广泛应用于深度学习。

### AdamW

Adam的一个变体，主要解决了权重衰减的正确实现问题。

- 在原始Adam中，权重衰减（L2正则化）通常与梯度计算耦合在一起。而AdamW将其解耦，在更新参数时，将权重衰减项单独分离出来。
- 比标准Adam具有更好的泛化性能，在现代架构（如Transformer）中已成为默认选择。
