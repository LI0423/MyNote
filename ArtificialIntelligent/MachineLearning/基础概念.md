# 基础概念

## 梯度、梯度爆炸、梯度消失

### 梯度

一元函数中，梯度就是函数的导数。

多元函数中，梯度是一个向量，描述了一个多元函数在其定义域内某一点所有变量上的方向导数的最大值。

梯度不仅包含每个变量的偏导数，还提供了函数在给定点上增长最快的方向（梯度的方向）以及增长的速率（梯度的模长）。通过梯度下降算法，可以计算损失函数相对于模型参数的梯度，并依据梯度的负方向更新参数，从而减少损失并优化模型性能。

### 梯度消失

当误差信号从网络的最后一层反向传播到前面的层时，由于链式法则作用下的梯度连乘效应，导致梯度随着网络深度的增加而急剧减小，直至趋于零。

在反向传播算法中，每个神经元的梯度是由其后一层所有神经元的梯度与它们之间的权重相乘后累加的导的。若激活函数的梯度在某些区域非常平缓，并且网络层级较多时，连续多次的乘积运算将很容易导致梯度累积效果趋于零。网络前端的层几乎接收不到有效的梯度信号，进而无法进行有效的参数更新，这些层的学习就几乎停止了。

#### 梯度消失后果

1. 参数收敛困难。
2. 学习能力丧失。
3. 训练失败。

#### 梯度消失解决办法

1. 激活函数的选择：避免使用梯度饱和的激活函数，改用ReLU及其变体。
2. 权重初始化：采用合理的权重初始化策略。
3. 批量归一化：BN层能在每层训练过程中对输入数据做标准化处理，有效解决内部协变量偏移问题，有助于稳定梯度的传播。
4. 残差结构：在深度残差网络中，直接添加从某层到后续层的快捷连接，使梯度可以直接绕过一些非线性变换，从而解决深度网络中的梯度消失问题。
5. 门控机制（LSTM、GRU）：在循环神经网络中，通过设计门控单元来控制信息流，以防止长期依赖关系中的梯度消失。

### 梯度爆炸

梯度爆炸是指在反向传播过程中，梯度值变得极大，以至于在更新网络权重时产生极端的、不稳定的变动。

在计算神经网络中各层参数梯度时，由于某种原因（如权重矩阵的元素异常大、学习率设置不当、激活函数的导数过大等），在反向传播中经过多层连续的乘法操作后，梯度的大小迅速增加到非常大的数值，甚至超出计算机浮点数表示范围，发生梯度爆炸。

#### 梯度爆炸后果

1. 数值不稳定：梯度值过大时，更新权重时会产生巨大的步长，使模型参数瞬间跳到远离最优解的位置，训练过程变得非常不稳定。
2. NaN问题：当梯度值过大以至于溢出时，可能会导致数值计算结果变为NaN，意味着训练过程崩溃，无法继续进行。
3. 过度拟合：可能导致模型过于复杂且对训练数据极为敏感。

#### 梯度爆炸解决办法

1. 梯度裁剪：这是一种直接干预梯度大小的技术，当梯度的范数超过预设阈值时，将其重新缩放到合理的范围内。
2. 权重约束与正则化，可以通过权重衰减（L2正则化）等方式来限制权重的增长，或者在训练中强制权重满足特定约束，如权重矩阵的范数不超过一定值。
3. 学习率调度：合理设置学习率，并随着训练过程动态调整学习率，可以避免因初始学习率过大而导致的梯度爆炸。

## 前向传播过程

前向传播是指神经网络在接收到输入数据时，数据在网络中从输入层经过隐藏层直至输出层的整个计算流程。

1. 输入层：输入数据（图像的像素值、文本的词嵌入向量）被送入网络的输入层。
2. 隐藏层：数据在进入网络后，每一层神经元都会对其接收到的数据进行加权求和，再加上一个偏置项，然后通过一个激活函数（ReLU、sigmoid、tanh等）来引入非线性，生成该层的输出，即下一层的输入。
3. 层间传递：这一过程在隐藏层之间递归进行，每一层神经元的输出作为下一层神经元的输入。
4. 输出层：经过一系列隐藏层的处理后，数据到达输出层。输出层的激活值代表了模型对输入数据的预测结果，如分类问题中的各类别的概率，或者是回归问题中的预测值。
5. 计算损失：前向传播的最后阶段通常会计算模型预测输出与实际目标值之间的差异，即损失函数的值，这个值反映了模型预测的好坏程度。

## 超参数（学习率、批次、层数）和参数（权重、偏置）

### 参数（Parameters）

参数是指模型内部学习到的变量，这些变量是在训练过程中通过最小化损失函数自动调整的。直接决定了模型的功能和预测能力。

神经网络中的权重（weights）和偏置（bias）就是典型的参数。

#### 参数调整方法

1. 梯度下降：
   1. 批量梯度下降（Batch Gradient Descent）：使用整个数据集来计算梯度。
   2. 随机梯度下降（Stochastic Gradient Descent，SGD）：每次仅用一个样本来估计梯度，速度快但不稳定。
   3. 小批量梯度下降（Mini-batch Gradient Descent）：结合了上面两种方法的优点，使用了一个小批次的数据来计算梯度。
   4. 动量（Momentum）、Adagrad、RMSprop、Adam等。
2. 正则化技术：
   1. L1/2正则化：通过惩罚较大的权重来防止过拟合。
   2. Dropout：随机忽律部分神经元，减少复杂模型的过拟合风险。
   3. Early Stopping：当验证集上的性能不再提升时提前终止训练。
3. 初始化策略：
   1. 适当的权重初始化可以帮助加快收敛速度，并有助于避免梯度消失或爆炸问题。

### 超参数（Hyperparameters）

超参数是关于模型结构或训练过程的配置选项，不能通过训练过程直接学到，而是在训练开始前由开发者根据经验、实验或特定策略来设置。

- 学习率（Learning Rate，LR）：决定了参数更新的速度。
- 批次大小（Batch Size）：每次迭代使用的样本数量。
- 网络层数（Number of Layers）和每层神经元数（Number of Neurons per Layer）。
- 正则化系数（Regularization Coefficients）等。

#### 超参数调整方法

1. 手动搜索
2. 网格搜索：通过遍历给定的参数网格，对每一组参数进行交叉验证，并选择平均性能最好的参数组合。
3. 随机搜索：在参数空间中随机选择参数组合，进行交叉验证，并选择平均性能最好的参数组合。
4. 贝叶斯优化：使用概率模型来预测超参数的性能，然后选择最有可能性的参数进行组合评估。
5. 进化算法：模仿自然选择过程，通过对候选解决方案群体进行变异、交叉和选择操作逐步逼近最优解。
6. 自动化超参数优化过程。

## 过拟合和欠拟合

- 欠拟合是指模型在训练集、验证集和测试集上均表现不佳的情况；
- 过拟合是指模型在训练集上表现很好，到了验证集和测试阶段就很差，即模型的泛化能力很差。

### 原因

- 欠拟合：
  1. 模型复杂度过低。
  2. 特征量过少。
- 过拟合：
  1. 建模样本选取有误，导致选取的样本数据不足以代表预定的分类规则；

## 独热编码

独热编码（One-Hot Encoding）是一种将分类或者离散型特征转化为数值型特征的方式。在独热编码中，每一个可能的类别或状态都会被表示为一个二进制向量，向量的长度等于所有可能类别的总数。对于每一个具体的样本，其所属类别的向量位置会被赋值为1，而其他位置则为0。

例如一个特征有个可能的类别：红、绿、蓝。对于一个样本，如果是绿色，那么独热编码后会得到一个三维向量[0, 1, 0]，第二个位置代表绿色。

### 优点

1. 消除了类别之间潜在的顺序关系，每个类别都有自己独立的维度，不存在数值上的大小比较。
2. 便于模型直接处理，许多机器学习算法可以直接接受这种二进制形式的输入。
3. 明确区分每个类别，使得模型能够针对性地学习每个类别的特征。

当类别非常多时，特征空间的维度会急剧增加，从而产生稀疏矩阵和存储需求增大等问题。

## 词嵌入序列

词嵌入序列是指在自然语言处理（NLP）中，将一段文本中的词汇通过词嵌入（Word Embedding）技术转化为一串连续的稠密向量序列的过程。

1. 对于文本中的每个单词，首先利用预训练好的词嵌入模型（Word2Vec、GloVe或BERT等）将其转换为一个固定维度的向量，这个向量能够编码该单词在语义空间中的信息。
2. 当完成文本中所有单词的词嵌入转换后，将这些单词对应的向量串联起来，形成一个向量序列。

## Baseline architectures（基准架构）

基准架构是指在深度学习领域中，经过广泛验证和应用的一些预训练神经网络模型，可以作为其他任务的起点或基础。通常具有较高的准确性和鲁棒性。

- VGG：VGG是一种基于卷积神经网络的图像分类模型。使用了多个卷积层和池化层，并且每个卷积层的卷积核大小和步长都是固定的。
- ResNet：一种基于残差连接的卷积神经网络模型。通过引入残差连接来解决深度神经网络中的梯度消失问题，使得网络可以更深层次地学习特征。
- Inception：一种基于多尺度卷积的卷积神经网络模型，通过使用不同大小的卷积核来学习不同尺度的特征，并且使用多个卷积层来捕捉不同层次的特征。
- BERT：一种基于Transformer的预训练语言模型。
- GPT：一种基于Transformer的预训练语言模型。

## token

指代原始输入数据经过预处理后的基本单元。NLP中文本序列中最基本的离散单元，通常是一个词语、字符、子词（subward）或者是单词的一部分。

## 预训练和微调

- 预训练模型（Pre-trained Model）是指在大规模未标注数据上首先进行训练以获得通用知识和模式的模型。
- 微调模型（Fine-tuning Model）是将预训练模型应用到具体任务上时，针对特定目标或问题进一步调整模型参数的过程。

预训练模型提供了一个具有广泛适用性的基础框架，通过对大规模未标注数据的学习获得了丰富的通用特征表达；而微调模型是在预训练模型的基础上针对具体任务进行针对性训练，使其更贴合实际应用场景的需求。

## 模型参数量

大模型的参数量指的是模型中权重（w）和偏置（b）的数量。权重决定了神经元连接的强度，偏置决定了神经元的激活阈值。存储了模型从数据中学习到的知识和经验，也决定了模型的智能和性能。
