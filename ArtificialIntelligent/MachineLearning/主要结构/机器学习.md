# 机器学习

## 张量

张量是一个数据容器，包含的数据几乎总是数值数据，矩阵是二维张量，张量是矩阵向任意维度的推广，张量的维度（Dimension）通常叫作轴（axis）

### 标量（0D张量）

仅包含一个数字的张量叫作标量（scalar，也叫标量张量、零维张量、0D张量）。在Numpy中，一个float32或float64的数字就是一个标量张量。可以用ndim属性来查看一个Numpy张量的轴的个数。标量张量有0个轴（ndim==0）。张量的个数也叫作阶（rank）。

```python
import numpy as np
x = np.array(12)
print(x.ndim)
```

### 向量（1D张量）

数字组成的数组叫作向量（vector）或一维张量（1D张量）。一维张量只有一个轴。

```python
import numpy as np
x = np.array([12,3,6,14,7])
print(x.ndim)
```

这个向量有5个元素，所以被称为5D向量。5D向量只有1个轴，沿着轴有5个维度，而5D张量有5个轴（沿着每个轴可能有任意个维度）。维度可以表示沿着某个轴上的元素个数，也可以表示张量中轴的个数，更准确的说法是5阶张量。

### 矩阵（2D张量）

向量组成的数组叫作矩阵（matrix）或二维张量（2D张量）。矩阵有2个轴（通常叫作行和列）。

```python
import numpy as np
x = np.array([5,78,2,34,0],
                [6,79,3,35,1],
                [7,80,4,36,2])
print(x.ndim)
```

### 3D张量与更高维张量

将多个矩阵组合成一个新的数组，可以得到一个3D张量，可以理解为数字组成的立方体。

```python
import numpy as np 
x = np.array([[[5,78,2,34,0],
                [6,79,3,35,1],
                [7,80,4,36,2]],
                [[5,78,2,34,0],
                [6,79,3,35,1],
                [7,80,4,36,2]],
                [[5,78,2,34,0],
                [6,79,3,35,1],
                [7,80,4,36,2]]])
```

将多个3D张量组合成一个数组，可以创建一个4D张量，以此类推。深度学习处理的一般是0D到4D的张量，处理视频数据时可能会遇到5D张量。

### 关键属性

- 轴的个数（阶）。
- 形状。是一个整数元组，表示张量沿着每个轴的维度大小（元素个数）。前面矩阵的形状是（3，5），3D张量的形状是（3，3，5），向量的形状只包含一个元素（5，），标量的形状为空（）。
- 数据类型（在Python库中通常叫作dtype）。是张量中所包含数据的类型，张量的类型可以是float32、unit8、float64等，在极少数情况下会遇到字符（char）张量。Numpy中不存在字符串张量，因为张量存储在预先分配的连续内存段中，而字符串的长度是可变的，无法用这种方式存储。

### 向量数据

最常见的数据，每个数据点都被编码为一个向量，因此一个数据批量就被编码为2D张量，其中第一个轴是样本轴，第二个轴是特征轴。

### 时间序列数据或序列数据

当时间（或序列顺序）对于数据很重要时，应该将数据存储在带有时间轴的3D张量中。每个样本可以被编码为一个向量序列，因此一个数据批量就被编码为一个3D张量，时间轴始终是第二个轴（索引为1的轴）。

### 图像数据

图像数据通常具有三个维度：高度、宽度和颜色深度。虽然灰度图像只有一个颜色通道，因此可以保存在2D张量中，但按照惯例，图像张量始终都是3D张量，灰度图像的彩色通道只有一维。因此如果图像大小为256x256，那么128张灰度图像可以保存在一个形状为（128，256，256，1）的张量中，而128张彩色图像组成的批量都可以保存在一个形状为（128，256，256，3）的张量。图像张量形状有两种约定：通道在后（channels-last）的约定（TensorFlow中使用）和通道在前（channels-first）的约定。tensorflow机器学习将颜色深度轴放在最后（samples，height，width，color_depth）。Theano将图像深度轴放在批量轴之后： (samples, color_depth, height, width) 。Keras框架同时支持两种格式。

### 视频数据

视频数据是现实生活中需要用到的5D张量的少数数据类型之一。视频可以看作一系列帧，每一帧都是一张彩色图像。每一帧都可以保存在一个形状为（height，width，color_depth）的3D张量中，因此一系列帧可以保存在一个形状为（frames，height，width，color_width）的4D张量中，而不同视频组成的批量则可以保存在一个5D张量中，其形状为（samples，frames，height，width，coloe_width）。

## 模型训练输出参数含义

- Epoch：训练过程中的迭代次数（即完成了多少个epoch）
- gpu_mem：GPU内存使用情况，通常是以MB或GB为单位的数字。
- box_loss：模型预测出的bounding box的平均损失值。
- cls_loss：模型预测出的分类的平均损失值。
- dfl_loss：所有损失总和，即box+cls。
- Instances：
  - Size：输入模型的图像的大小，通常是以像素为单位的宽度和高度。
  - P：该类别的预测精准度（precision），即正确预测的物体数量占所有预测的物体数量的比例。
  - R：该类别的召回率（recall），即正确预测的物体数量占所有真实物体数量的比例。
  - mAP@5：平均精度均值（mean average precision）的值，即IoU阈值为0.5时的平均精度。
  - mAP@5.95：在IoU阈值从0.5到0.95的范围内，所有阈值的平均精度的均值。

## 基本概念

1. 回归（regression）：通过输入的数据，让机器能够预测未来的数据，在这个过程中，找到可以预测未来的函数的任务叫做回归。
2. 分类（classification）：人类准备好一些选项，选项叫作类别（class），要找的函数的输出就是从设定好的选项里选择一个当作输出，该任务称作分类。
3. 结构化学习（structured learning）：机器产生一个有结构的物体，例如图片，文章，这种机器产生有结构的问题称作结构化学习。
4. 机器学习找函数的过程
   1. 写出一个带有未知参数的函数：y = b + w * x
      - b、w是未知的，带有未知参数（parameter）的函数称为模型（model）。特征（feature）是已知的x。
      - b、w是未知参数，w称为权重（weight），b称为偏置（bias）。
   2. 定义损失（loss）
      - 损失也是一个函数，损失是函数 L(b,w) 这个函数的输入是模型里的参数，输入是模型参数b和w。
      - 真实值 y 与预测值 ^y 之间绝对值的差距称为平均绝对误差（Mean Absolute Error，MAE），e = |^y - y|。
      - 真实值 y 与预测值 ^y 之间平方的差距称为均方误差（Mean Squared Error，MSE），e = (^y - y)^2。
      - 通过设定不同的参数，计算它的损失，画出来的等高线图称作误差表面（error surface）。在等高线图上，越偏红色系，代表计算出来的损失越大，这一组w跟b越差；
      - 越偏蓝色系，就代表损失越小，这一组w跟b越好，拿这一组w跟b放到函数里预测越精准。
   3. 解一个最优化的问题。

5. 线性模型（linear model）：把输入的特征x乘上一个权重，再加上一个偏置得到预测的结果，称为线性模型。
6. 分段线性曲线
7. 梯度下降

把N笔数据随机分成一个一个的批量（batch），每个批量里有B笔数据。原本是把所有的数据拿出来计算一个损失，现在是拿一个批量里的数据出来计算一个损失。每次选一个批量来计算L1，根据L1来计算梯度，用L1来更新参数；再选一个批量来计算L2，根据L2来计算梯度，再用L2来更新参数。所以并不是拿L来计算梯度，实际上是用L1，L2……来计算梯度。把所有的批量都过一次称为一个回合（epoch）。每次更新参数叫作一次更新，把所有的批量都看过一遍，叫作一个回合。
8. Sigmoid或ReLU称为神经元，很多的神经元称为神经网络（neural network）。
9. 过拟合（overfitting）：在训练数据和测试数据上的结果是不一致的。

解决问题的方向：

1. 数据增强（data augmentation），即增加训练集的数据量。就是根据问题的理解创造出新的数据。例如在做图像识别时，将数据集里的图像左右翻转，或者截一块出来放大等等。
2. 给模型一些限制，让模型不要有过大的灵活性。
   - 给模型比较少的参数。如果是深度学习的话，就给比较少的神经元的数量。
   - 用比较少的特征。本来给三天的数据，改成两天的数据。
   - 还有别的方法，早停（early stoping）、正则化（regularization）和丢弃法（dropout method）。
