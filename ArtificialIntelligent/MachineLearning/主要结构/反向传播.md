# 反向传播（Backpropagation）

在已知网络前向计算和损失后，高效地计算损失对网络中每个参数的偏导数（梯度），然后用这些梯度更新参数（梯度下降）。基于链式法则，把复杂计算图拆成小块，从输出向输入逐层传递梯度。

## 链式法则

若复合函数 $L=f(g(h(x)))$，则
$$\frac{dL}{dx} = \frac{dL}{df} · \frac{df}{dg} · \frac{dg}{dh} · \frac{dh}{dx}$$
在神经网络中，把每层视作一个函数节点，反向传播就是按这个乘积顺序把上游梯度乘回去。
