# 损失函数（Loss Function）

损失函数，也称为代价函数。是一个衡量模型预测值（Prediction）与真实值（True Value）之间差异程度的函数。为模型提供反馈信号，告诉模型现在差距有多大，从而指引模型应该向哪个方向、多大的力度去调整（通过优化算法）。

## 常见损失函数及应用场景

### 用于回归任务（预测连续值）

1. 均方误差（MSE）
   $$\frac{\sum_{i=1}^{N}(y_i - \hat{y}_i)^2}{N}$$
2. 平均绝对误差（MAE）
   $$\frac{\sum_{i=1}^{N}|y_i - \hat{y}_i|}{N}$$
3. Huber Loss
   $$Huber Loss = \begin{cases}
    \frac{(y - \hat{y})^2}{2}, 如果|y - \hat{y}| \le \delta \\
    \delta(|y - \hat{y}| - \frac{\delta}{2}), 如果|y - \hat{y}| \gt \delta
   \end{cases}$$

### 用于分类任务（预测离散类别）

1. 交叉熵损失（Cross-Entropy）
    $$-\frac{\sum_{i=1}^{N}y_i \log(\hat{y}_i)}{N}$$
2. 二元交叉熵（BCE）
    $$-\frac{\sum_{i=1}^{N} [y_i \log(\hat{y}_i) + (1- y_i)\log(1-\hat{y}_i)]}{N}$$
3. 合叶损失（Hinge Loss）
    $$\frac{\sum_{i=1}^{N}\max(0, 1 - y_i \cdot \hat{y}_i)}{N}， y_i\in{(-1, 1)}$$

## 均方误差（Mean Squared Error，MSE）

均方误差是机器学习中常用的损失函数之一，用于衡量模型的预测值与真实值之间的差异。适用于回归问题，旨在最小化模型的预测误差。

### MSE计算方法

有一个包含n个样本的数据集，其中每个样本的真实值为$y_i$，模型的预测值为$\hat{y}_i$，则均方误差为：
$$MSE = \frac{\sum_{i=1}^{N}(y_i - \hat{y}_i)^2}{N}$$

1. 首先计算每个样本的预测值与真实值之间的差异，即$(y_i - \hat{y}_i)$。
2. 然后将这些差异平方，以消除正负差异，得到$(y_i - \hat{y}_i)^2$。
3. 最后计算所有样本的平方差异的平均值，即求和并除以样本数量n，得到均方误差。

### MSE适用场景

均方误差通常用于回归问题，其中模型的目标是预测连续数值输出（例如，房价预测、销售量预测等）。均方误差对异常值（离群点）敏感，因为它对差异的平方进行了求和，因此在使用MSE时需要注意异常值的处理。

## 均方根误差（Root Mean Squared Error，RMSE）

均方根误差和均方误差类似，但是在计算前会对均方误差进行平方根运算，以测量模型的预测值与真实值之间的平均误差。

### RMSE计算方法

$$RMSE = \sqrt{\frac{\sum_{i=1}^{N}(y_i - \hat{y}_i)^2}{N}}$$

1. 首先计算每个样本的预测值与真实值之间的差异，即$(y_i - \hat{y}_i)$。
2. 然后将这些差异平方，以消除正负差异，得到$(y_i - \hat{y}_i)^2$。
3. 接下来计算所有样本的平方差异的平均值，即求和并除以样本数量n。
4. 最后对均方误差取平方根，得到均方根误差RMSE。

### RMSE适用场景

均方根误差通常用于回归问题，特别是在希望更好地理解模型的预测误差的情况下，因为它以与原始数据相同的度量单位表示误差。与MSE不同，RMSE对误差的量级更为敏感，因此可以用于比较不同问题中模型的性能。

## 平均绝对误差（Mean Absolute Error，MAE）

平均绝对误差用于回归问题中衡量模型的预测值与真实值之间的平均绝对差异。

### MAE计算方法

$$MAE = \frac{\sum_{i=1}^{N}|y_i - \hat{y}_i|}{N}$$

1. 对于每个样本，计算模型的预测值与真实值之间的差异，即$|\hat{y}_i - y_i|$。
2. 对这些差异取绝对值，以确保都为正数。
3. 计算所有样本的绝对差异的平均值，求和并除以样本数量n，得到平均绝对误差MAE。

### MAE适用场景

平均绝对误差适用于回归问题，特别适用于数据中存在离群值（outliers）的情况，因为它对异常值的影响较小。与均方误差（MSE）和均方根误差（RMSE）不同，MAE不会对误差进行平方运算，因此更为稳健，对异常值不敏感。

## 交叉熵损失（Cross-Entropy Loss）

交叉熵损失（Cross-Entropy Loss），也称为对数损失（Logarithmic Loss），是机器学习中用于分类问题的一种常见损失函数。它用于衡量模型的分类输出与真实标签之间的差异。

### 交叉熵损失计算方法

$$-\frac{\sum_{i=1}^{N}y_i \log(\hat{y}_i)}{N}$$

1. 对于每个样本，计算真实标签$y_i$与模型输出$\hat{y}_i$的交叉熵。这是通过将真实标签的one-hot编码与模型输出的概率值相乘并取对数得到的。
2. 对所有样本的交叉熵进行求和，并除以样本数量n，得到平均交叉熵损失。

### 交叉熵损失适用场景

交叉熵损失通常用于分类问题，特别是在多类别分类问题中。它对模型的分类结果与真实标签之间的误差进行有效的建模，激励模型产生更准确的分类概率分布。交叉熵损失在深度学习中广泛应用，特别是在图像分类、文本分类、语音识别等任务中。

## 多分类交叉熵损失函数（Categorial Cross-Entropy）

比较两个概率分布的差异。损失值越小，说明预测分布与真实分布越接近。

### 公式

$$L = -\sum_{i=1}^C y_i · \log(\hat{y_i})$$

- $C$：类别的总数。
- $y_i$：真实标签中第$i$个类别的值（要么是0，要么是1）。
- $\hat{y}_i$：模型预测出的第$i$个类别的概率（经过Softmax后）。
- $\log$：通常指自然对数（ln）。

### 示例

如果真实标签是：[1, 0, 0]代表“猫”，而预测概率为：[0.659, 0.242, 0.099]。
将真实分布和预测概率代入公式得到：
$L = -[1 · \log(0.659) + 0 · \log(0.242) + 0 · \log(0.009)] = - \log(0.659) \approx 0.417$

### 为何多分类交叉熵如此有效

假设有一个真实的类别 $k$（即$y_k = 1$），模型对它的预测概率是 $\hat{y}_k$。经过推导后，损失函数$L$关于第$j$个类别的logit（$z_j$，即Softmax的输入）的梯度为：
$\frac{\delta L}{\delta z_j} = \hat{y_j} - y_j$

这个公式意味着，梯度 = 预测值 - 真实值。指出了参数需要调整的方向和幅度。

- 对于真实类别$k$：梯度是$(\hat{y}_k - 1)$，这是一个负数，告诉网络需要增加这个类别的logit值。
- 对于其他所有错误类别$j$：梯度是 $(\hat{y}_j - 0) = \hat{y}_j$。这是一个正数，告诉网络需要减少这些类别的logit值。

### 优点

1. 梯度永不饱和：无论预测概率是0.99还是0.01，梯度$(\hat{y}_j - y_j)$都是一个合理值，不会变得极小。彻底解决Sigmoid等函数带来的梯度消失问题。
2. 收敛速度快：误差信号非常直接和巨大。模型如果犯了一个明显的错误（如$\hat{y}_{true} = 0.1$），会立刻收到一个很大的梯度$(-0.9)$，从而进行大幅度的参数更新来快速修正错误。

## Hinge损失

Hinge损失是机器学习中用于支持向量机等分类算法的损失函数。它用于衡量模型的分类输出与真实标签之间的差异，并被设计为一种较好的分离超平面优化目标。

### Hinge计算方法

$$\frac{\sum_{i=1}^{N}\max(0, 1 - y_i \cdot \hat{y}_i)}{N}， y_i\in{(-1, 1)}$$

1. 对于每个样本，计算模型的分类输出$f(x_i)$与真实标签$y_i$之间的差异，即$1 - y_i \cdot \hat{y}_i$。
2. 使用Hinge损失函数，取差异与0的最大值，即$max(0, 1 - y_i \cdot \hat{y}_i)$。
3. 对所有样本的Hinge损失进行求和，并除以样本数量n，得到平均Hinge损失。

### Hinge适用场景

Hinge损失通常用于支持向量机（SVM）等二分类算法中，它鼓励模型产生具有更大边距的分类超平面，并对误分类的样本有惩罚。

Hinge损失在处理线性可分和线性不可分的数据集时都有效，但在训练过程中需要小心调整超参数，以平衡边距和误分类样本的权衡。

## Huber损失

Huber损失是一种平滑的损失函数，通常用于回归问题，以降低对异常值（离群点）的敏感性。与均方误差（MSE）相比，Huber损失对离群点具有更好的鲁棒性，能够在一定程度上减少异常值的影响。

### Huber计算方法

$$Huber Loss = \begin{cases}
\frac{(y - \hat{y})^2}{2}, 如果|y - \hat{y}| \le \delta \\
\delta(|y - \hat{y}| - \frac{\delta}{2}), 如果|y - \hat{y}| \gt \delta
\end{cases} $$

1. 如果是真实值$y$和模型的预测值$\hat{y}$之间的差异$|y - \hat{y}|$小于等于阈值$\delta$，则使用平方差的一半来计算损失。
2. 如果$|y - \hat{y}|$大于阈值$\delta$，则计算一个线性部分，该部分使损失函数以恒定的斜率增加，以减小对离群点的敏感性。

### Huber适用场景

Huber损失通常用于回归问题，特别是在数据中存在离群值（异常值）的情况下。与均方误差（MSE）相比，Huber损失更加鲁棒，能够在一定程度上减少离群值对模型的影响。Huber损失的阈值参数$\delta$可以根据问题的特性进行调整，以控制对离群值的敏感度。

## Kullback-Leibler散度

Kullback-Leibler（KL）散度，也称为相对熵，是一种用于衡量两个概率分布之间差异的度量。KL散度通常用于衡量两个概率分布之间的相似性或差异性。

### KL散度计算方法

假设有两个概率分布P和Q，KL散度的计算方法如下：
$$KL(P||Q) = \sum_i P(i)log\left(\frac{P(i)}{Q(i)}\right)$$

1. 对于每个事件i，计算$P(i)$与$Q(i)$的比值，然后取自然对数。
2. 将这个比值乘以$P(i)$。
3. 对所有事件的结果求和，得到KL散度。

KL散度是非对称的，即$KL(P||Q)$与$KL(Q||P)$通常不相等。度量的是将分布$Q$用于近似分布$P$时所引入的信息损失。

### KL散度适用场景

KL散度在机器学习中的许多领域都有应用，包括：
- 概率模型比较：用于比较两个概率分布，例如在生成模型评估中。
- 信息理论：KL散度是信息论中的一个重要概念，用于度量信息的差异。
- 正则化：在正则化损失函数中，KL散度可以作为正则项，帮助模型更好地拟合数据。
