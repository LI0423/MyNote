# 奖惩机制

在模型训练中，惩罚机制（Penalty）和奖励机制（Reward）并不是两个独立的系统，而是同一枚硬币的两面，都通过损失函数和优化过程来实现。

## 惩罚机制

当模型出现我们不希望的行为时，就通过损失函数对其施加一个“惩罚”（增加损失值），模型为了最小化损失，就会被迫减少这种行为。

### 实现方式

#### L1/L2正则化

- 不希望的行为：模型过于复杂、权重值过大（容易导致过拟合）。
- 如何惩罚：在原始的损失函数（如交叉熵）后面直接添加一个惩罚项。
  - L2正则化（权重衰减）：惩罚项为 $\lambda * \sum{w_i^2}$。倾向于产生小而分散的权重，使函数更加平滑。
  - L1正则化：惩罚项为 $\lambda * \sum{|w_i|}$。倾向于产生稀疏权重，即直接将一些不重要的特征的权重压到0，具有特征选择的功能。
- 实现：在现代深度学习框架的优化器里，直接设置weight_decay参数即可实现L2正则化。

#### Dropout

- 不希望的行为：神经元之间产生复杂的协同适应，即过度依赖某些特定的神经元（导致过拟合）。
- 如何惩罚：训练时随机让网络中的一部分神经元失活（输出置为0）。
- 实现：在网络中直接添加Dropout层。

#### 提前停止

- 不希望的行为：模型在训练集上表现继续提升，但在验证集上性能开始下降（过拟合）。
- 如何惩罚：当监测到验证集损失不再下降反而连续上升时，就提前终止训练过程，惩罚了后续会导致过拟合的后续训练步骤。

## 奖励机制

当模型做出了期望的行为（预测正确）时，就减少损失值作为一种“奖励”，模型为了持续获得这种奖励，就会倾向于重复这种行为。

### 实现方式

#### 损失函数本身

- 期望的行为：模型将高概率分配给正确的类别。
- 如何奖励：当模型对正确类别的预测概率$\hat{y}_{true}$升高时，交叉熵损失$L = -\log(\hat{y}_{true})$会急剧下降。这种损失值的下降本身就是对模型的一种巨大“奖励”，鼓励下次做出同样的预测。

#### 强化学习

- 期望的行为：智能体（Agent）再环境中采取了一个好的动作。
- 如何奖励：环境会直接给出一个显性的奖励信号，例如得分增加。这个奖励信号被用来构造一个目标函数，通过策略梯度等算法，最大化这个期望奖励。
