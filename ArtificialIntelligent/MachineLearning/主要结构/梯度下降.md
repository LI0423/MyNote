# 梯度下降

梯度下降时一种用于寻找函数最小值的迭代式优化算法。

## 梯度

### 定义

多元函数全部偏导数所构成的向量。使用倒三角符号$\nabla$表示某个函数的梯度：
$$\nabla f(x,y)=(\frac{\delta f}{\delta x}, \frac{\delta f}{\delta y})$$
$$\nabla g(x,y,z)=(\frac{\delta g}{\delta x}, \frac{\delta g}{\delta y}, \frac{\delta g}{\delta z})$$

梯度向量就是函数对其自变量求偏导后组成的向量。表示在各个方向上函数变化的最快方向及其变化率。

### 性质

梯度向量 $\nabla f$指向函数值增长最快的方向，其模长$||\nabla f||$表示这个最大增长率。

- 梯度方向：函数上升最陡的方向。
- 梯度大小：这个方向的斜率。
- 负梯度方向：函数下降最快的方向。

对于函数上某一点：

- 如果沿着函数梯度的正方向运动，函数值增加的最快。
- 如果沿着函数梯度的反方向运动，函数值减小的最快。

## 核心思想

有一个目标函数（损失函数或成本函数），衡量了模型预测值与真实值之间的误差。想找到一组模型参数（比如权重w和偏置b），使这个目标函数的值最小。梯度下降通过不断向函数值下降最快的方向（即梯度的反方向）调整参数，根据学习率的大小，一步一步逼近最小值点。

## 更新公式

$$\theta = \theta - \mu · \nabla J(\theta)$$

- $\theta$：代表模型的参数（例如权重w和偏置b）。
- $\mu$：学习率。超参数，控制着我们每次更新参数的步长。
  - 太小：下山速度太慢，需要很多步才能收敛，训练时间过长。
  - 太大：步长过大，可能会一步跨过最低点，导致无法收敛，甚至在最小值附近来回震荡或发散。
- $\nabla J(\theta)$：损失函数J在当前位置$\theta$的梯度。
- 负号：想要最小化损失函数，要朝着梯度（上升最快）的反方向（下降最快）移动。

## 算法步骤

1. 初始化：随机初始化模型参数$\theta$（给w和b随机赋一些小的值）。
2. 循环迭代：
   1. 计算梯度：在当前参数$\theta$，计算损失函数的梯度$\nabla J(\theta)$。
   2. 更新参数：使用公式$\theta = \theta - \mu · \nabla J(\theta)$更新所有参数。
   3. 检查收敛：判断是否满足停止条件（例如，梯度值接近零、损失函数下降幅度小于某个阈值，或者达到了预设的最大迭代次数）。
3. 输出：返回最终优化后的参数$\theta$。

## 三种类型

### 批量梯度下降

- 做法：在每一步更新中，都使用整个训练数据集来计算梯度。
- 优点：
  - 由于使用了全部数据，计算的梯度方向非常准确，能直接朝向全局最优解（如果存在）。
  - 收敛过程稳定。
- 缺点：
  - 当数据集非常大时，计算一次梯度需要遍历所有数据，速度极慢，内存可能无法承受。
  - 无法进行在线学习（模型无法再获取数据实时更新）。

### 随机梯度下降

- 做法：在每一步更新中，随机选取一个训练样本来计算梯度，并立即更新参数。
- 优点：
  - 速度非常快，每次只计算一个样本的梯度。
  - 可以用于在线学习。
  - 由于其随机性，有可能跳出局部最小值。
- 缺点：
  - 梯度方向波动很大，损失函数会剧烈震荡。虽然整体趋势是下降的，但路径很曲折。
  - 可能永远也无法精确收敛到全局最小值，而是在其附近徘徊。

### 小批量梯度下降

- 做法：前两种方法的折中。在每一步更新中，使用一个小批量的数据来计算梯度。
- 优点：
  - 兼具了BGD的稳定性和SGD的速度。
  - 可以通过矩阵运算进行并行化，充分利用GPU的计算能力，计算效率高。
- 目前在实际应用和深度学习中最常用、最主流的方法。通常说的梯度下降指的就是小批量梯度下降。
