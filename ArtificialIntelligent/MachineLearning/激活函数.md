# 激活函数（Activation Function）

## 核心作用

1. 引入非线性
    - 激活函数在每个神经元后引入了非线性变换，使得神经网络可以无限逼近任何复杂的非线性函数。让神经网络成了一个“万能函数逼近器”，能够学习数据中极其复杂的模式和关系。
2. 决定神经元是否被激活
    - 激活函数决定了来自上层神经元的输入信号（加权求和后的结果 $z = w * x + b$）是否足够重要，以至于应该没传递到下一层。
    - 根据输入$z$的大小，输出一个决定信号强弱的数值。例如ReLU函数在z<0时输出0，可以理解为“关闭”了这个神经元；在z>0时输出z，可以理解为“按原样激活”了这个神经元。

## 常见激活函数

### Sigmoid

- 公式：$f(z) = \frac{1}{1 + e^{-z}}$
- 输出范围：(0, 1)
- 优点：
  - 输出平滑，易于求导。
  - 将输出压缩到(0, 1)，可以解释为概率。
- 缺点：
  - 梯度消失（Vanishing Gradient）：当输入z的绝对值很大时，函数的梯度（导数）回变得非常小，接近于0。在反向传播时，梯度会层层相乘，导致靠前的网络层梯度几乎为0，参数无法有效更新。
  - 输出非零中心：Sigmoid的输出恒大于0，导致后续神经元的输入全部为正，在反向传播时权重w的梯度要么全正，要么全负，优化路径呈“之”字形，降低收敛效率。
  - 计算成本高：涉及指数运算。
- 应用场景：很少用于隐藏层，主要用于二分类问题的输出层，将输出解释为概率。

### Tanh

- 公式：$f(z)=\frac{e^z - e^{-z}}{e^z + e^{-z}}$
- 输出范围：(-1, 1)
- 优点：
  - 输出是零中心化的，解决了Sigmoid的一部分优化问题。
  - 相比Sigmoid，其输出范围更大，梯度更强。
- 缺点：
  - 仍然存在梯度消失问题。
  - 同样涉及指数运算，计算成本高。
- 应用场景：在RNN（如LSTM、GRU）中较为常见，在CNN中已被ReLU系列所取代。

### ReLU（Rectified Linear Unit，修正线性单元）

- 公式：$f(z) = max(0, z)$
- 输出范围：$[0, + \infty)$
- 优点：
  - 缓解梯度消失：在正区间，梯度恒为1，彻底解决梯度消失问题。
  - 计算速度极快：只需要一个阈值判断，没有复杂的指数运算。
- 缺点：
  - Dead ReLU Problem（死亡ReLU问题）：当输入z < 0时，梯度为0。意味着一个神经元落入负区间，权重可能永远无法更新，神经元再也不会被激活（也就是死亡）。
  - 输出非零中心。
- 应用场景：目前最常用、默认的激活函数，尤其适用于CNN和深层网络的隐藏层。

### Leaky ReLU & PReLU（渗漏型/参数化ReLU）

- 公式：$f(z) = max(\alpha z, z)$
  - Leaky ReLU：$\alpha$是一个很小的固定值（如0.01）。
  - PReLU（Parametric ReLU）：$\alpha$是一个可学习的参数。
- 优点：
  - 解决了死亡ReLU问题。当 z < 0 时，仍然有一个很小的梯度$\alpha$，神经元有机会被重新激活。
- 缺点：
  - 效果不总是稳定，且引入了一个需要tuning或学习的超参数$\alpha$
- 应用场景：可以尝试在ReLU效果不佳时使用，尤其是在非常深的网络中。

### ELU（Exponential，指数线性单元）

- 公式：$f(z) = \begin{cases}
    z, & \text{if } z \ge 0 \\
    \alpha(e^z - 1), & \text{if } z < 0
\end{cases}$
- 优点：
  - 兼具ReLU的优点（正区间无饱和）
  - 处理负输入时输出平滑渐近于$- \alpha$，避免了死亡ReLU问题。
  - 是零中心化的，有助于加速训练。
- 缺点：
  - 计算涉及指数运算，比ReLU慢。
- 应用场景：在需要更高精度和稳定性的任务中表现良好，但计算成本较高。

### Swish

- 公式：$f(z) = z \cdot \sigma(z) = \frac{z}{1 + e^{-z}}$，其中的$\sigma$是sigmoid函数。
- 特点：
  - 在许多深度学习任务上优于ReLU。
  - 是一条平滑、非单调的曲线。无上界、有下界。
  - 类似于ReLU，有一个平滑的过渡，而不是硬转折。
- 优点：
  - 平滑的特性使得优化landscapes更平滑，有助于梯度的流动，通常能获得更好的泛化性能。
- 缺点：
  - 计算成本较高（包含sigmoid）。
- 应用场景：正在成为ReLU的有力竞争者，尤其在视觉和NLP任务中。

### Softmax

- 公式：$\sigma(z)_i = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}} for i = 1, 2, ..., K$
  - 对一个包含K个类别的分类问题，给定一个包含K个元素的logits向量 $z = [z_1, z_2, ..., z_k]$
  - 取指数（Exponentiate）：对每个logit $z_i$计算 $e^{z_i}$。将所有的分数转换为正数，因为概率不能为负。
  - 求和：计算所有指数化后的值之和：$sum = e^{z_1} + e^{z_2} + ... + e^{z_k}$。
  - 归一化：将每个指数化的值除以总和。确保所有结果的和为1，从而形成一个有效的概率分布。
- 优点：
  - 输出是概率分布：所有输出值在(0,1)范围内，且总和为1。完美契合多分类问题需求。
  - 放大效应：由于指数函数特性，Softmax会拉开不同logits之间的差距。最高的logit值对应的概率会主导整个分布，最低的值则会被抑制到接近0。
  - 可解释性：输出可以直接解释为每个类别的概率。
  - 与交叉熵损失完美配合：
- 实现细节：在实际应用中，直接计算 $e^{z_i}$ 可能遇到数值溢出的问题（如果 $e^{z_i}$ 很大，可能超出计算机能表示的最大浮点数）。
  - 解决方案：使用一个数学技巧来稳定计算。
  - $\sigma(z)_i = \frac{e^{z_{i} - C}}{\sum_{j=1}^K e^{z_j - C}}$
  - 令常数C=max(z)，即减去logits向量中的最大值。
  - 防止溢出：最大指数项变为 $e^{max(z) - max(z)} = e^0 = 1$，避免计算一个巨大的数。
  - 结果不变：分子分母同时减去一个常数，比值保持不变。

## 如何选择激活函数

|场景|推荐选择|理由|
|---|---|---|
|隐藏层|ReLU|简单高效，是可靠的起点|
|需要更高性能|Swish或Leaky ReLU|在许多基准测试表现优于ReLU|
|处理死亡ReLU问题|Leaky ReLU/ PReLU / ELU |为负输入提供梯度，避免神经元死亡|
|二分类输出层|Sigmoid|输出范围(0, 1)，天然解释为概率|
|多分类输出层|Softmax|将输出归一化为概率分布|
|回归任务输出层|Linear（恒等函数）或ReLU|直接输出任意值或正值|
|RNN/LSTM|Tanh|传统常用，零中心化有助于控制信号流|
