# 线性变换与非线性变换

## 线性变换

### 数学定义

一个变换$L$是线性的，当且仅当满足以下两个条件：

- 可加性：$L(x + y) = L(x) + L(y)$
- 齐次性：$L(\alpha x) = \alpha L(x)$

就是先加再变和先变再加的结果是一样的；先缩放再变和先变再缩放的结果也是一样的。

### 表现形式

线性代数中，线性变换通常通过矩阵乘法来实现。$y = Ax + b$

- $A$是变换矩阵（负责旋转、缩放、剪切）
- $x$是输入向量
- $b$是偏置向量（负责平移）
- $y$是输出向量

严格来说，$y = Ax + b$被称为仿射变换。仿射变换是线性变换（$y = Ax$）加上一个平移操作（$+ b$）。在机器学习中，通常把仿射变换笼统地称为线性变换。

### 几何直观与性质

- 原点不一定保持不变：因为可能有平移$b$。
- 直线变换后仍是直线。
- 平行线变换后仍保持平行。
- 比例保持不变：在线上的点，变换后其比例关系不变。

## 非线性变换

### 数学定义

不满足线性变换的两个条件的变换就是非线性变换。$f(\alpha x + \beta y) \ne \alpha f(x) + \beta f(y)$

### 表现形式

非线性变换没有统一的一个矩阵形式，可以是任何复杂的函数。$y = f(x)$，其中$f$是一个非线性函数，如$\sin(x), x^2, \log(x), max(0, 1)$等。

### 几何直观与性质

- 直线可以变成曲线。
- 平行线可能变得不平行。
- 可以产生全新的复杂形状和模式。
- 比例关系会被打破。

## 为什么在深度学习中至关重要

神经网络可以看作是一系列变换的堆叠：
$$y = f_{3}(W_3 * f_{2}(W_2 * f_{1}(W_1 * x + b_1) + b_2) + b_3)$$

其中 $W * x + b$是线性变换，$f()$ 是非线性激活函数。

### 只有线性变换

如果神经网络只包含线性变换（去掉所有非线性激活函数），无论堆叠多少层，整个网络都会退化成一个单一的线性变换。
$$Y = W_3 * (W_2 * (W_1 * x + b_1) + b_2) + b_3$$

意味着深层网络失去了意义，根本无法学习复杂的数据模式，能力和一个线性回归模型没有区别。

### 非线性变换的作用

非线性激活函数的引入打破了这种线性退化，使神经网络每一层的变换都有了独特的“性格”。神经网络可以：

1. 学习复杂模式：通过组合非线性变换，网络可以拟合极其复杂的函数，从而理解图像、语言、声音中的非线性关系。
2. 成为通用函数逼近器：一个足够深的带有非线性激活函数的神经网络可以任意精度逼近任何复杂函数。
