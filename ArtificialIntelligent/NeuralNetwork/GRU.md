# GRU（Gated Recurrent Unit，门控循环单元）

GRU是RNN的变体，主要是为了解决RNN梯度消失/爆炸而产生的。可以更好地捕捉时间序列中长距离的依赖关系。

## 核心思想

用门来控制信息流。门是一种让信息选择性地通过的结构，由一个Sigmoid神经网络层和一个逐点乘法操作组成。

- Sigmoid层：输出一个介于0和1之间的数值。描述了应该让多少信息通过。
  - 0 代表完全不允许任何信息通过。
  - 1 代表让所有信息完全通过。

## 数学公式

$$z_t=\sigma(W_z · [h_{t-1}, x_t] + b_z)$$

- 更新门：决定了有多少过去的信息 $h_{t-1}$ 需要传递到未来。
  - 如果$z_t$接近1，说明会保留大部分过去的记忆。
  - 如果$z_t$接近0，说明会丢弃大部分过去的记忆。

$$r_t=\sigma(W_r · [h_{t-1}, x_t] + b_z)$$

- 重置门：决定了需要忘记多少过去的信息，以便计算新的候选记忆。
  - 如果$r_t$接近0，表示要重置或忘记之前的隐藏状态，认为之前的信息对计算当前候选状态不重要。
  - 如果$r_t$接近1，表示会保留大部分之前的信息。

$$\hat{h_t}=tanh(W · [r_t * h_{t-1}, x_t] + b)$$

- 计算候选隐藏状态
