# 决策树（Decision Tree）

决策树是一种以树形结构展示决策规则和分类结果的模型。作为一种归纳算法模型，可以从看似无序、杂乱的数据中提取出数据分类的规律，并利用其进行新数据分类或预测。

决策树可被用于解决分类和回归问题。

## 结构

- 根节点：代表整个数据集的起点，是所有特征的集合。
- 内部节点：代表一个特征或属性上的测试。
- 分支：代表一个测试的结果，连接两个节点。
- 叶节点：也叫终端节点，代表最终的决策结果，不再继续划分。

## 核心算法

决策树的核心是如何选择节点、用哪个特征进行划分。目标是让划分后的数据子集尽可能地“纯”。

### 衡量不纯度的指标

- 信息熵
  - 概念：来自信息论，表示系统的混乱程度。熵越大，信息越混乱（不纯度越高）；熵越小，系统越有序（纯度越高）。
  - 公式：$Entropy(S) = -\sum_{i=1}^c p_i log_2 p_i$
    - $S$是当前的数据集/
    - $c$是类别的数量。
    - $p_i$是第$i$个类别在数据集$S$中所占的比例。
- 基尼不纯度
  - 概念：衡量从一个数据集中随机抽取两个样本，其类别标签不一致的概率。概率越小，纯度越高。
  - 公式：$Gini(S) = 1 - \sum_{i=1}^c p_i^2$

### 最佳划分特征的指标

- 信息增益
  - 概念：使用某个特征划分数据集前后，信息熵的减少量。信息增益越大，说明这个特征划分后，纯度提升得越多，该特征就越好。
  - 公式：$IG(S,A) = Entropy(S) - \sum_{v \in Values(A)} \frac{|S_v|}{S} Entropy(S_v)$
    - $A$是某个特征。
    - $Values(A)$是特征$A$所有取值的集合。
    - $S_v$是特征A取值为$v$的子集。
  - 缺点：信息增益倾向于选择取值较多的特征，因为划分得太细，每个子集都很纯，会导致过拟合。
- 增益率
  - 概念：为了解决信息增益的缺点，对取值多的特征进行惩罚。C4.5算法使用它。
  - 公式：$GainRatio(S,A) = \frac{IG(S,A)}{IntrinsicValue(S,A)}$
- 基尼增益
  - 概念：与信息增益类似，使用的是基尼不纯度。CART算法使用它。

## 代码示例

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import matplotlib.pyplot as plt


# 加载数据
iris = load_iris()
X = iris.data
y = iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建决策树分类器，并设置参数以防止过拟合
# criterion：选择分割质量的度量标准，'gini'为基尼不纯度，'entropy'为信息增益。
# max_depth：树的最大深度
clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)

# 训练模型
clf.fix(X_train, y_train)

# 评估模型
accuracy = clf.score(X_test, y_test)
print(f"模型在测试集上的准确率: {accuracy:.2f}")
```

## 决策树剪枝

一种用于减小树的复杂度、防止过拟合的技术。主要是通过修建决策树的一部分来降低模型的复杂度，从而提高模型的泛化能力。

### 预剪枝

常用方法：

- 设置一个最大深度，当树达到该深度时停止分裂。
- 设置节点分裂的最小样本数，如果样本数少于某个阈值，则不再分裂。
- 设置节点分裂的最小增益，如果信息增益（或基尼增益）小于某个阈值，则不再分裂。

### 后剪枝

先让决策树完全生长，然后自底向上对非叶节点进行考察，如果将该节点对应的子树替换为叶节点能带来泛化性能的提升，则将该子树替换为叶节点。

常用方法：

- 错误率降低剪枝
- 悲观错误剪枝
- 代价复杂度剪枝（最小误差剪枝）

## 随机森林

集成学习：通过构建并结合多个机器学习模型来改善模型的性能。通过训练多个模型，并将他们的预测结果进行某种方式的结合，通常可以得到比单一模型更好的预测结果。

Bagging：是Bootstrap aggregating的缩写，通过结合多个模型的预测结果来减少模型方差。每个模型都是在原始数据集的随机子集上训练的，这些随机子集是通过有放回的抽样得到的。然后所有模型的预测结果通过投票（分类问题）或平均（回归问题）的方式进行结合。

随机森林：一种特殊的Bagging方法，用于改进单个决策树模型的性能。其中每个模型都是一个决策树。每个决策树还在每个节点处从随机子集中选择最佳分裂。

### 训练过程

1. 初始化模型参数
2. 构建决策树
   1. 样本抽样：如果bootstrap=True，则使用自助采用从训练集中有放回地抽取样本，形成每棵树的训练集，未被抽中的样本称为袋外（Out-of-Bag，OOB）样本。
   2. 特征选择：在每个节点的分裂过程中，随机选择max_features数量的特征作为候选特征。
   3. 最佳分裂点选择：在候选特征中，选择一个特征和阈值来最大化节点的纯度（使用基尼不纯度或信息增益）。
   4. 递归分裂：重复特征选择和最佳分裂点选择的过程，直到达到最大深度或叶节点的最小样本数。
3. 决策树训练

   每棵树都从根节点开始，递归地分裂知道满足停止条件：
   1. 内部叶节点分裂：在内部节点，根据选定的特征和阈值分裂样本。
   2. 叶节点生成：当节点满足停止条件时，将其标记为叶节点，并根据多数类或平均值确定预测结果。
4. 聚合结果

   随机森林的最终预测结果是通过聚合所有决策树的预测结果得到的：
   1. 分类任务：对于每个样本，所有树对其进行分类，并采用多数投票法确定最终类别。
   2. 回归任务：对于每个样本，所有树给出预测值，并计算这些预测值的平均值作为最终预测结果。

两个重要的随机性：

1. 数据采集的随机性：每个决策树模型都是在随机的子数据集上进行训练的，有助于减少过拟合风险。
2. 特征选取的随机性：在每个节点分裂时，随机选择一部分特征进行计算，有助于增加模型的多样性。

### 袋外数据

随机森林中的袋外数据，是指在构建随机森林模型时，由于自助采样过程中产生的那些没有被选中用于训练个别决策树的样本。在自助采样中，每次从原始训练数据集中随机抽取一个样本，抽样后将样本放回，每个样本都有被重复抽取的机会。在构建每棵树时大约有36.8%的样本不会被选中，未被选中的样本就构成了袋外数据。

袋外数据可以用来估计模型的泛化误差，而不需要单独的验证集。对于每棵决策树，可以用未选中的袋外数据来计算预测误差，如分类错误率或回归的均方误差。误差的平均值可以作为模型性能的一个无偏估计，可以用来验证模型的稳定性和可靠性。一般情况很少使用OBB，数据量大时，会显著增加训练的时间复杂度。

### 随机森林代码示例

1. 分类

    ```python
    from sklearn.datasets import load_iris
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score

    iris = load_iris()
    X = iris.data
    y = iris.target

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)

    rf_classifier.fit(X_train, y_train)

    y_pred = rf_classifier.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(f'Accuracy: {accuracy:.2f}')

    feature_importances = rf_classifier.feature_importances_
    print('Feature importance:', feature_importances)
    ```

2. 回归

    ```python
    from sklearn.datasets import load_iris
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score

    iris = load_iris()
    X = iris.data
    y = iris.target

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=3, random_state=42)

    rf_regressor.fit(X_train, y_train)

    y_pred = rf_regressor.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(f'Accuracy: {accuracy:.2f}')

    feature_importances = rf_classifier.feature_importances_
    print('Feature importance:', feature_importances)
    ```

### 缺点

1. 预测速度慢：由于需要集成多棵树的预测，不进行并行运算的话，随机森林的预测速度可能较慢。
2. 内存消耗大：随机森林需要存储多棵树，因此内存消耗较大。
3. 对噪声数据敏感：随机森林可能会过度拟合噪声数据，尤其是在特征数量较少时。
4. 可能欠拟合：如果树的数量太少或者树的深度太浅，随机森林可能会欠拟合。
