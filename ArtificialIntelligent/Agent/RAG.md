# RAG（Retrievval-Augment-Generation，检索增强生成）

将额外信息存储为向量，将传入的查询与这些向量匹配，并将最相似的信息与查询一起传递给大语言模型。

RAG流程：分片 -> 索引（提问前） -> 召回 -> 重排 -> 生成（提问后）

![RAG流程](images/RAG.webp)

## 分片

将一个当文档切分成多个片段。分片有以下方式：

### 固定大小切分

最直观的切分方法是根据预定的字符数、单词数或Token数量将文本均匀分割成若干段落。直接切分可能破坏语义的流程性，所以在连续段落间保留一些重叠。

该方法通常会在句子（或想法）中途切分，导致重要信息可能分散在不同段落中。

![固定大小切分](images/固定大小切分.webp)

### 语义切分

根据句子、段落或主题部分等有意义的单元来切分文档，接着位每个段落生成嵌入。如果第一个段落的嵌入和第二个段落的嵌入余弦相似度比较高，那么这两个段落组成一个切片。这个过程持续进行，直到余弦相似度显著下降。一旦下降就开始一个新切片重复该过程。

该方法能够保持语言的自然流畅性，并保留完整的思想。问题是确定余弦相似度的阈值在不同的文档中可能不同。

![语义切分](images/语义切分.webp)

### 递归切分

首先基于内在的分隔符（段落或章节）进行切分。然后某个切片的大小超过预定义的切片大小限制，就做进一步分割。如果符合切片大小规则就不再进行切分。

该方法也保持了语言的自然流畅性，并保留了完整的思想。不过在实现和计算复杂性方面有一些额外的开销。

![递归切分](images/递归切分.webp)

### 文档结构切分

利用文档内在结构（标题、章节或段落）定义切片边界。

该方法能够保持文档结构的完整性，确保切片与文档逻辑部分对齐。

该方法假设文档结构清晰，但是可能并非如此。切片长度可能不同，甚至超过模型的token限制，可以与递归切片结合使用。

![文档结构切分](images/文档结构切分.webp)

### LLM切分

LLM可以通过提示词生成语义隔离且有意义的切片。

该方法确保了高语义准确性，因为LLM能理解上下文和意义，远超简单的启发式方法。这种方法的计算成本是最高的，并且LLM通常有上下文窗口限制。

![LLM切分](images/LLM切分.webp)

## 索引

1. 通过Embedding将片段文本转换为向量。
2. 将片段文本和片段向量存储到向量数据库中。

### Embedding

将文本转换为向量的一个过程。含义相近的文本在经过Embedding之后，对应的向量也是相近的。

Embedding操作通常是由专门的Embedding模型来完成的。mteb排行榜可以看哪个模型好用。

### 向量数据库

用于存储和查询向量的数据库。不仅需要存储向量，也需要存储原始文本。目的是通过向量可以获取到对应的原始文本。

## 召回

搜索与用户问题相关片段的过程。

用户问题 -> Embedding模型 -> 问题向量 -> 向量数据库 -> 用户问题相关片段。

### 多路召回

在RAG系统的检索阶段，同时使用多种不同的检索策略或算法，从知识库中获取相关文档片段，然后将各路的结果进行融合、去重和重排序，最后再将最优质、最全面的结果传递给大模型生成答案。

#### 实现步骤

1. 构建多个检索通道：选择多个检索策略或者模型，如传统的关键词检索、基于语义的检索（BERT）、基于用户画像的检索等。
2. 检索并返回候选项：使用每个检索通道独立进行检索，并返回一批候选项。
3. 候选项的融合与排序：对不同通道返回的候选项进行融合，使用重排序算法对候选项进行排序，确保最终返回最相关的结果。
4. 生成最终答案：基于排序后的结果，生成最终的答案或文档。

### 向量相似度计算方法

- 余弦相似度：计算两个向量之间夹角的cos值，根据cos值判断夹角大小，夹角越小，相似度越高。
- 欧氏距离：计算两个向量之间的距离，距离越小，相似度越高。
- 点积：通过代数方式衡量两个向量相似度，不仅考虑向量的方向关系，也要考虑向量的长度。

### 相似度计算方法特点

- 成本低。
- 耗时短。
- 准确率低。

适合初步筛选。

## 重排

从召回结果中挑选与用户问题最相似的结果作为重排的结果。使用cross-encoder模型计算每个片段和用户问题的相似度。

## 生成

将用户问题和重排后获取的相似片段一同发送给大语言模型，让大模型根据片段内容来回答用户问题。
